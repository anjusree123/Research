{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvnWfITfjbdMFeerZQoYNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjusree123/Research/blob/main/Classification_BSEloss_Crossentropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J77-q5QHtouB"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import FakeData\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating network\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 3, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "gxGJdH0juILI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  # Prepare FakeData dataset\n",
        "  dataset = FakeData(size=15000, image_size=(3, 28, 28), num_classes=2, transform=transforms.ToTensor())\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers = 4, pin_memory = True)\n",
        "\n",
        "  # Initialize the MLP\n",
        "  mlp = MLP()\n",
        "\n",
        "  # Define the loss function and optimizer\n",
        "  loss_function = nn.BCELoss()\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "  # Run the training loop\n",
        "  for epoch in range(0, 5): # 5 epochs at maximum\n",
        "\n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "    # Set current loss value\n",
        "    current_loss = 0.0\n",
        "\n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "      # Get inputs\n",
        "      inputs, targets = data\n",
        "\n",
        "      # Prepare targets\n",
        "      targets = targets \\\n",
        "                  .type(torch.FloatTensor) \\\n",
        "                  .reshape((targets.shape[0], 1))\n",
        "\n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = loss_function(outputs, targets)\n",
        "\n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print statistics\n",
        "      current_loss += loss.item()\n",
        "      if i % 10 == 0:\n",
        "          print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 500))\n",
        "          current_loss = 0.0\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io7qmbAbeorc",
        "outputId": "9d1d7f07-ed8b-4083-f0d9-c2efccaa30b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.014\n",
            "Loss after mini-batch    21: 0.014\n",
            "Loss after mini-batch    31: 0.014\n",
            "Loss after mini-batch    41: 0.014\n",
            "Loss after mini-batch    51: 0.014\n",
            "Loss after mini-batch    61: 0.014\n",
            "Loss after mini-batch    71: 0.014\n",
            "Loss after mini-batch    81: 0.014\n",
            "Loss after mini-batch    91: 0.014\n",
            "Loss after mini-batch   101: 0.014\n",
            "Loss after mini-batch   111: 0.014\n",
            "Loss after mini-batch   121: 0.014\n",
            "Loss after mini-batch   131: 0.014\n",
            "Loss after mini-batch   141: 0.014\n",
            "Loss after mini-batch   151: 0.014\n",
            "Loss after mini-batch   161: 0.014\n",
            "Loss after mini-batch   171: 0.014\n",
            "Loss after mini-batch   181: 0.014\n",
            "Loss after mini-batch   191: 0.014\n",
            "Loss after mini-batch   201: 0.014\n",
            "Loss after mini-batch   211: 0.014\n",
            "Loss after mini-batch   221: 0.014\n",
            "Loss after mini-batch   231: 0.014\n",
            "Starting epoch 2\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.014\n",
            "Loss after mini-batch    21: 0.014\n",
            "Loss after mini-batch    31: 0.014\n",
            "Loss after mini-batch    41: 0.014\n",
            "Loss after mini-batch    51: 0.014\n",
            "Loss after mini-batch    61: 0.014\n",
            "Loss after mini-batch    71: 0.014\n",
            "Loss after mini-batch    81: 0.014\n",
            "Loss after mini-batch    91: 0.014\n",
            "Loss after mini-batch   101: 0.014\n",
            "Loss after mini-batch   111: 0.014\n",
            "Loss after mini-batch   121: 0.014\n",
            "Loss after mini-batch   131: 0.014\n",
            "Loss after mini-batch   141: 0.014\n",
            "Loss after mini-batch   151: 0.014\n",
            "Loss after mini-batch   161: 0.014\n",
            "Loss after mini-batch   171: 0.014\n",
            "Loss after mini-batch   181: 0.014\n",
            "Loss after mini-batch   191: 0.014\n",
            "Loss after mini-batch   201: 0.014\n",
            "Loss after mini-batch   211: 0.014\n",
            "Loss after mini-batch   221: 0.014\n",
            "Loss after mini-batch   231: 0.014\n",
            "Starting epoch 3\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.014\n",
            "Loss after mini-batch    21: 0.014\n",
            "Loss after mini-batch    31: 0.014\n",
            "Loss after mini-batch    41: 0.014\n",
            "Loss after mini-batch    51: 0.014\n",
            "Loss after mini-batch    61: 0.014\n",
            "Loss after mini-batch    71: 0.014\n",
            "Loss after mini-batch    81: 0.014\n",
            "Loss after mini-batch    91: 0.014\n",
            "Loss after mini-batch   101: 0.014\n",
            "Loss after mini-batch   111: 0.014\n",
            "Loss after mini-batch   121: 0.014\n",
            "Loss after mini-batch   131: 0.014\n",
            "Loss after mini-batch   141: 0.014\n",
            "Loss after mini-batch   151: 0.014\n",
            "Loss after mini-batch   161: 0.014\n",
            "Loss after mini-batch   171: 0.014\n",
            "Loss after mini-batch   181: 0.014\n",
            "Loss after mini-batch   191: 0.014\n",
            "Loss after mini-batch   201: 0.014\n",
            "Loss after mini-batch   211: 0.014\n",
            "Loss after mini-batch   221: 0.014\n",
            "Loss after mini-batch   231: 0.014\n",
            "Starting epoch 4\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.014\n",
            "Loss after mini-batch    21: 0.014\n",
            "Loss after mini-batch    31: 0.014\n",
            "Loss after mini-batch    41: 0.014\n",
            "Loss after mini-batch    51: 0.014\n",
            "Loss after mini-batch    61: 0.014\n",
            "Loss after mini-batch    71: 0.014\n",
            "Loss after mini-batch    81: 0.014\n",
            "Loss after mini-batch    91: 0.014\n",
            "Loss after mini-batch   101: 0.014\n",
            "Loss after mini-batch   111: 0.014\n",
            "Loss after mini-batch   121: 0.014\n",
            "Loss after mini-batch   131: 0.014\n",
            "Loss after mini-batch   141: 0.014\n",
            "Loss after mini-batch   151: 0.014\n",
            "Loss after mini-batch   161: 0.014\n",
            "Loss after mini-batch   171: 0.014\n",
            "Loss after mini-batch   181: 0.014\n",
            "Loss after mini-batch   191: 0.014\n",
            "Loss after mini-batch   201: 0.014\n",
            "Loss after mini-batch   211: 0.014\n",
            "Loss after mini-batch   221: 0.014\n",
            "Loss after mini-batch   231: 0.014\n",
            "Starting epoch 5\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.014\n",
            "Loss after mini-batch    21: 0.014\n",
            "Loss after mini-batch    31: 0.014\n",
            "Loss after mini-batch    41: 0.014\n",
            "Loss after mini-batch    51: 0.014\n",
            "Loss after mini-batch    61: 0.014\n",
            "Loss after mini-batch    71: 0.014\n",
            "Loss after mini-batch    81: 0.014\n",
            "Loss after mini-batch    91: 0.014\n",
            "Loss after mini-batch   101: 0.014\n",
            "Loss after mini-batch   111: 0.014\n",
            "Loss after mini-batch   121: 0.014\n",
            "Loss after mini-batch   131: 0.014\n",
            "Loss after mini-batch   141: 0.014\n",
            "Loss after mini-batch   151: 0.014\n",
            "Loss after mini-batch   161: 0.014\n",
            "Loss after mini-batch   171: 0.014\n",
            "Loss after mini-batch   181: 0.014\n",
            "Loss after mini-batch   191: 0.014\n",
            "Loss after mini-batch   201: 0.014\n",
            "Loss after mini-batch   211: 0.014\n",
            "Loss after mini-batch   221: 0.014\n",
            "Loss after mini-batch   231: 0.014\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BCEWithLogitsLoss**\n",
        "No need to add sigmoid as sigmoid is already in logits"
      ],
      "metadata": {
        "id": "AD1k57HefQ3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import FakeData\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 3, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "COHXAS11fQTC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  # Prepare FakeData dataset\n",
        "  dataset = FakeData(size=15000, image_size=(3, 28, 28), num_classes=2, transform=transforms.ToTensor())\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers = 4, pin_memory = True)\n",
        "\n",
        "  # Initialize the MLP\n",
        "  mlp = MLP()\n",
        "\n",
        "  # Define the loss function and optimizer\n",
        "  loss_function = nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "  # Run the training loop\n",
        "  for epoch in range(0, 5): # 5 epochs at maximum\n",
        "\n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "    # Set current loss value\n",
        "    current_loss = 0.0\n",
        "\n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "      # Get inputs\n",
        "      inputs, targets = data\n",
        "\n",
        "      # Prepare targets\n",
        "      targets = targets \\\n",
        "                  .type(torch.FloatTensor) \\\n",
        "                  .reshape((targets.shape[0], 1))\n",
        "\n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = loss_function(outputs, targets)\n",
        "\n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print statistics\n",
        "      current_loss += loss.item()\n",
        "      if i % 10 == 0:\n",
        "          print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 500))\n",
        "          current_loss = 0.0\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R20exeuYflbn",
        "outputId": "7d32b45a-d1da-435b-8597-4b3d7f49a26f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.014\n",
            "Loss after mini-batch    21: 0.014\n",
            "Loss after mini-batch    31: 0.014\n",
            "Loss after mini-batch    41: 0.014\n",
            "Loss after mini-batch    51: 0.014\n",
            "Loss after mini-batch    61: 0.014\n",
            "Loss after mini-batch    71: 0.014\n",
            "Loss after mini-batch    81: 0.014\n",
            "Loss after mini-batch    91: 0.014\n",
            "Loss after mini-batch   101: 0.014\n",
            "Loss after mini-batch   111: 0.014\n",
            "Loss after mini-batch   121: 0.014\n",
            "Loss after mini-batch   131: 0.014\n",
            "Loss after mini-batch   141: 0.014\n",
            "Loss after mini-batch   151: 0.014\n",
            "Loss after mini-batch   161: 0.014\n",
            "Loss after mini-batch   171: 0.014\n",
            "Loss after mini-batch   181: 0.014\n",
            "Loss after mini-batch   191: 0.014\n",
            "Loss after mini-batch   201: 0.014\n",
            "Loss after mini-batch   211: 0.014\n",
            "Loss after mini-batch   221: 0.014\n",
            "Loss after mini-batch   231: 0.014\n",
            "Starting epoch 2\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.014\n",
            "Loss after mini-batch    21: 0.014\n",
            "Loss after mini-batch    31: 0.014\n",
            "Loss after mini-batch    41: 0.014\n",
            "Loss after mini-batch    51: 0.014\n",
            "Loss after mini-batch    61: 0.014\n",
            "Loss after mini-batch    71: 0.014\n",
            "Loss after mini-batch    81: 0.014\n",
            "Loss after mini-batch    91: 0.014\n",
            "Loss after mini-batch   101: 0.014\n",
            "Loss after mini-batch   111: 0.014\n",
            "Loss after mini-batch   121: 0.014\n",
            "Loss after mini-batch   131: 0.014\n",
            "Loss after mini-batch   141: 0.014\n",
            "Loss after mini-batch   151: 0.014\n",
            "Loss after mini-batch   161: 0.014\n",
            "Loss after mini-batch   171: 0.014\n",
            "Loss after mini-batch   181: 0.014\n",
            "Loss after mini-batch   191: 0.014\n",
            "Loss after mini-batch   201: 0.014\n",
            "Loss after mini-batch   211: 0.014\n",
            "Loss after mini-batch   221: 0.014\n",
            "Loss after mini-batch   231: 0.014\n",
            "Starting epoch 3\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.014\n",
            "Loss after mini-batch    21: 0.014\n",
            "Loss after mini-batch    31: 0.014\n",
            "Loss after mini-batch    41: 0.014\n",
            "Loss after mini-batch    51: 0.014\n",
            "Loss after mini-batch    61: 0.014\n",
            "Loss after mini-batch    71: 0.014\n",
            "Loss after mini-batch    81: 0.014\n",
            "Loss after mini-batch    91: 0.014\n",
            "Loss after mini-batch   101: 0.014\n",
            "Loss after mini-batch   111: 0.014\n",
            "Loss after mini-batch   121: 0.014\n",
            "Loss after mini-batch   131: 0.014\n",
            "Loss after mini-batch   141: 0.014\n",
            "Loss after mini-batch   151: 0.014\n",
            "Loss after mini-batch   161: 0.014\n",
            "Loss after mini-batch   171: 0.014\n",
            "Loss after mini-batch   181: 0.014\n",
            "Loss after mini-batch   191: 0.014\n",
            "Loss after mini-batch   201: 0.014\n",
            "Loss after mini-batch   211: 0.014\n",
            "Loss after mini-batch   221: 0.014\n",
            "Loss after mini-batch   231: 0.014\n",
            "Starting epoch 4\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.014\n",
            "Loss after mini-batch    21: 0.014\n",
            "Loss after mini-batch    31: 0.014\n",
            "Loss after mini-batch    41: 0.014\n",
            "Loss after mini-batch    51: 0.014\n",
            "Loss after mini-batch    61: 0.014\n",
            "Loss after mini-batch    71: 0.014\n",
            "Loss after mini-batch    81: 0.014\n",
            "Loss after mini-batch    91: 0.014\n",
            "Loss after mini-batch   101: 0.014\n",
            "Loss after mini-batch   111: 0.014\n",
            "Loss after mini-batch   121: 0.014\n",
            "Loss after mini-batch   131: 0.014\n",
            "Loss after mini-batch   141: 0.014\n",
            "Loss after mini-batch   151: 0.014\n",
            "Loss after mini-batch   161: 0.014\n",
            "Loss after mini-batch   171: 0.014\n",
            "Loss after mini-batch   181: 0.014\n",
            "Loss after mini-batch   191: 0.014\n",
            "Loss after mini-batch   201: 0.014\n",
            "Loss after mini-batch   211: 0.014\n",
            "Loss after mini-batch   221: 0.014\n",
            "Loss after mini-batch   231: 0.014\n",
            "Starting epoch 5\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.014\n",
            "Loss after mini-batch    21: 0.014\n",
            "Loss after mini-batch    31: 0.014\n",
            "Loss after mini-batch    41: 0.014\n",
            "Loss after mini-batch    51: 0.014\n",
            "Loss after mini-batch    61: 0.014\n",
            "Loss after mini-batch    71: 0.014\n",
            "Loss after mini-batch    81: 0.014\n",
            "Loss after mini-batch    91: 0.014\n",
            "Loss after mini-batch   101: 0.014\n",
            "Loss after mini-batch   111: 0.014\n",
            "Loss after mini-batch   121: 0.014\n",
            "Loss after mini-batch   131: 0.014\n",
            "Loss after mini-batch   141: 0.014\n",
            "Loss after mini-batch   151: 0.014\n",
            "Loss after mini-batch   161: 0.014\n",
            "Loss after mini-batch   171: 0.014\n",
            "Loss after mini-batch   181: 0.014\n",
            "Loss after mini-batch   191: 0.014\n",
            "Loss after mini-batch   201: 0.014\n",
            "Loss after mini-batch   211: 0.014\n",
            "Loss after mini-batch   221: 0.014\n",
            "Loss after mini-batch   231: 0.014\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Negative log likelihood loss**\n",
        "Used when there is are more classes"
      ],
      "metadata": {
        "id": "pnA5J72BgbNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nn.NLLLoss uses a Softmax activated output in our neural network. nn.LogSoftmax is faster than pure nn.Softmax\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 1, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 10),\n",
        "      nn.LogSoftmax(dim = 1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "PRza4skrgTAx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  # Prepare MNIST dataset\n",
        "  dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "\n",
        "  # Initialize the MLP\n",
        "  mlp = MLP()\n",
        "\n",
        "  # Define the loss function and optimizer\n",
        "  loss_function = nn.NLLLoss()\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "  # Run the training loop\n",
        "  for epoch in range(0, 5): # 5 epochs at maximum\n",
        "\n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "    # Set current loss value\n",
        "    current_loss = 0.0\n",
        "\n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "      # Get inputs\n",
        "      inputs, targets = data\n",
        "\n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = loss_function(outputs, targets)\n",
        "\n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print statistics\n",
        "      current_loss += loss.item()\n",
        "      if i % 500 == 499:\n",
        "          print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 500))\n",
        "          current_loss = 0.0\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePi8VtUkjy4I",
        "outputId": "35e37c65-0f10-4352-80c9-dbc1c3df4c56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /content/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 143611632.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-images-idx3-ubyte.gz to /content/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /content/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 77058329.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /content/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 42609151.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 24175797.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 1.987\n",
            "Loss after mini-batch  1000: 1.118\n",
            "Loss after mini-batch  1500: 0.699\n",
            "Loss after mini-batch  2000: 0.561\n",
            "Loss after mini-batch  2500: 0.490\n",
            "Loss after mini-batch  3000: 0.451\n",
            "Loss after mini-batch  3500: 0.420\n",
            "Loss after mini-batch  4000: 0.409\n",
            "Loss after mini-batch  4500: 0.402\n",
            "Loss after mini-batch  5000: 0.374\n",
            "Loss after mini-batch  5500: 0.368\n",
            "Loss after mini-batch  6000: 0.340\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 0.315\n",
            "Loss after mini-batch  1000: 0.337\n",
            "Loss after mini-batch  1500: 0.316\n",
            "Loss after mini-batch  2000: 0.312\n",
            "Loss after mini-batch  2500: 0.290\n",
            "Loss after mini-batch  3000: 0.322\n",
            "Loss after mini-batch  3500: 0.288\n",
            "Loss after mini-batch  4000: 0.293\n",
            "Loss after mini-batch  4500: 0.294\n",
            "Loss after mini-batch  5000: 0.291\n",
            "Loss after mini-batch  5500: 0.294\n",
            "Loss after mini-batch  6000: 0.270\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 0.265\n",
            "Loss after mini-batch  1000: 0.287\n",
            "Loss after mini-batch  1500: 0.255\n",
            "Loss after mini-batch  2000: 0.266\n",
            "Loss after mini-batch  2500: 0.254\n",
            "Loss after mini-batch  3000: 0.255\n",
            "Loss after mini-batch  3500: 0.262\n",
            "Loss after mini-batch  4000: 0.245\n",
            "Loss after mini-batch  4500: 0.249\n",
            "Loss after mini-batch  5000: 0.242\n",
            "Loss after mini-batch  5500: 0.242\n",
            "Loss after mini-batch  6000: 0.239\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 0.232\n",
            "Loss after mini-batch  1000: 0.247\n",
            "Loss after mini-batch  1500: 0.233\n",
            "Loss after mini-batch  2000: 0.228\n",
            "Loss after mini-batch  2500: 0.205\n",
            "Loss after mini-batch  3000: 0.220\n",
            "Loss after mini-batch  3500: 0.232\n",
            "Loss after mini-batch  4000: 0.210\n",
            "Loss after mini-batch  4500: 0.217\n",
            "Loss after mini-batch  5000: 0.202\n",
            "Loss after mini-batch  5500: 0.228\n",
            "Loss after mini-batch  6000: 0.218\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 0.200\n",
            "Loss after mini-batch  1000: 0.206\n",
            "Loss after mini-batch  1500: 0.210\n",
            "Loss after mini-batch  2000: 0.193\n",
            "Loss after mini-batch  2500: 0.196\n",
            "Loss after mini-batch  3000: 0.208\n",
            "Loss after mini-batch  3500: 0.205\n",
            "Loss after mini-batch  4000: 0.184\n",
            "Loss after mini-batch  4500: 0.179\n",
            "Loss after mini-batch  5000: 0.184\n",
            "Loss after mini-batch  5500: 0.190\n",
            "Loss after mini-batch  6000: 0.193\n",
            "Training process has finished.\n"
          ]
        }
      ]
    }
  ]
}